{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Satellite Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1 : Install all the packages needed in cmd :-  tensorflow, tensorflow_addons, tensorflow_datasets, tensorflow_hub, numpy, matplotlib and sckitlearn using:\n",
    "### py -m pip install [packagename]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Install all the libraries needed from those packeges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Import the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the whole dataset, for data info\n",
    "\n",
    "all_ds   = tfds.load(\"eurosat\", with_info=True)\n",
    "\n",
    "# load training, testing & validation sets, splitting by 60%, 20% and 20% respectively\n",
    "\n",
    "train_ds = tfds.load(\"eurosat\", split=\"train[:60%]\")\n",
    "test_ds  = tfds.load(\"eurosat\", split=\"train[60%:80%]\")\n",
    "valid_ds = tfds.load(\"eurosat\", split=\"train[80%:]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Create variables for storing classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the class names\n",
    "class_names = all_ds[1].features[\"label\"].names\n",
    "# total number of classes (10)\n",
    "num_classes = len(class_names)\n",
    "num_examples = all_ds[1].splits[\"train\"].num_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Understanding the Data by plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a plot for number of samples on each class\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(14,10))\n",
    "labels, counts = np.unique(np.fromiter(all_ds[0][\"train\"].map(lambda x: x[\"label\"]), np.int32), \n",
    "                       return_counts=True)\n",
    "\n",
    "plt.ylabel('Counts')\n",
    "plt.xlabel('Labels')\n",
    "sns.barplot(x = [class_names[l] for l in labels], y = counts, ax=ax) \n",
    "for i, x_ in enumerate(labels):\n",
    "  ax.text(x_-0.2, counts[i]+5, counts[i])\n",
    "# set the title\n",
    "ax.set_title(\"Bar Plot showing Number of Samples on Each Class\")\n",
    "# save the image\n",
    "# plt.savefig(\"class_samples.png\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6: Preparing the data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_training(ds, cache=True, batch_size=64, shuffle_buffer_size=1000):\n",
    "  if cache:\n",
    "    if isinstance(cache, str):\n",
    "      ds = ds.cache(cache)\n",
    "    else:\n",
    "      ds = ds.cache()\n",
    "  ds = ds.map(lambda d: (d[\"image\"], tf.one_hot(d[\"label\"], num_classes)))\n",
    "  # shuffle the dataset\n",
    "  ds = ds.shuffle(buffer_size=shuffle_buffer_size)\n",
    "  # Repeat forever\n",
    "  ds = ds.repeat()\n",
    "  # split to batches\n",
    "  ds = ds.batch(batch_size)\n",
    "  # `prefetch` lets the dataset fetch batches in the background while the model\n",
    "  # is training.\n",
    "  ds = ds.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "  return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cache(): This method saves the preprocessed dataset into a local cache file. This will only preprocess it the very first time (in the first epoch during training).\n",
    "map(): We map our dataset so each sample will be a tuple of an image and its corresponding label one-hot encoded with tf.one_hot().\n",
    "shuffle(): To shuffle the dataset so the samples are in random order.\n",
    "repeat()Every time we iterate over the dataset, it'll repeatedly generate samples for us; this will help us during the training.\n",
    "batch(): We batch our dataset into 64 or 32 samples per training step.\n",
    "prefetch(): This will enable us to fetch batches in the background while the model is training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 7: Run the model for training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validating shapes\n",
    "for el in valid_ds.take(1):\n",
    "  print(el[0].shape, el[1].shape)\n",
    "for el in train_ds.take(1):\n",
    "  print(el[0].shape, el[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(64, 64, 64, 3) (64, 10)\n",
    "(64, 64, 64, 3) (64, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fantastic, both the training and validation have the same shape; where the batch size is 64, and the image shape is (64, 64, 3). The targets have the shape of (64, 10) as it's 64 samples with 10 classes one-hot encoded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 8: Building The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model_url = \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_l/feature_vector/2\"\n",
    "\n",
    "download & load the layer as a feature vector\n",
    "keras_layer = hub.KerasLayer(model_url, output_shape=[1280], trainable=True)\n",
    "\n",
    "m = tf.keras.Sequential([\n",
    "  keras_layer,\n",
    "  tf.keras.layers.Dense(num_classes, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "build the model with input image shape as (64, 64, 3)\n",
    "\n",
    "m.build([None, 64, 64, 3])\n",
    "m.compile(\n",
    "    loss=\"categorical_crossentropy\", \n",
    "    optimizer=\"adam\", \n",
    "    metrics=[\"accuracy\", tfa.metrics.F1Score(num_classes)]\n",
    ")\n",
    "m.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 9: Fine-Tuning The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"satellite-classification\"\n",
    "model_path = os.path.join(\"results\", model_name + \".h5\")\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(model_path, save_best_only=True, verbose=1)\n",
    "\n",
    "# set the training & validation steps since we're using .repeat() on our dataset\n",
    "# number of training steps\n",
    "n_training_steps   = int(num_examples * 0.6) // batch_size\n",
    "# number of validation steps\n",
    "n_validation_steps = int(num_examples * 0.2) // batch_size\n",
    "\n",
    "# train the model\n",
    "history = m.fit(\n",
    "    train_ds, validation_data=valid_ds,\n",
    "    steps_per_epoch=n_training_steps,\n",
    "    validation_steps=n_validation_steps,\n",
    "    verbose=1, epochs=5, \n",
    "    callbacks=[model_checkpoint]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 10: Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the best weights\n",
    "m.load_weights(model_path)\n",
    "\n",
    "# number of testing steps\n",
    "n_testing_steps = int(all_ds[1].splits[\"train\"].num_examples * 0.2)\n",
    "# get all testing images as NumPy array\n",
    "images = np.array([ d[\"image\"] for d in test_ds.take(n_testing_steps) ])\n",
    "print(\"images.shape:\", images.shape)\n",
    "# get all testing labels as NumPy array\n",
    "labels = np.array([ d[\"label\"] for d in test_ds.take(n_testing_steps) ])\n",
    "print(\"labels.shape:\", labels.shape)\n",
    "\n",
    "# feed the images to get predictions\n",
    "predictions = m.predict(images)\n",
    "# perform argmax to get class index\n",
    "predictions = np.argmax(predictions, axis=1)\n",
    "print(\"predictions.shape:\", predictions.shape)\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "accuracy = tf.keras.metrics.Accuracy()\n",
    "accuracy.update_state(labels, predictions)\n",
    "print(\"Accuracy:\", accuracy.result().numpy())\n",
    "print(\"F1 Score:\", f1_score(labels, predictions, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:\n",
    "\n",
    "Accuracy: 0.9677778\n",
    "F1 Score: 0.9655686619720163"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
